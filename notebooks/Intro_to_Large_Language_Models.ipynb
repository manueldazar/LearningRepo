{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction to LLMs and How To Use via API: Taking Control and Building a Conversation"
      ],
      "metadata": {
        "id": "pdyupkTqsz6j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the specific version of the openai library used in this lesson Because We pin\n",
        "# the version to ensure the code works exactly as shown, as library updates can sometimes\n",
        "# introduce changes.\n",
        "\n",
        "# The '-q' flag makes the installation quieter (less output)\n",
        "!pip install -q openai==1.59.8"
      ],
      "metadata": {
        "collapsed": true,
        "id": "eiqwUbfcShuQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44ebcc5a-57db-41c4-e695-bb728b21c4ae"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/455.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m455.6/455.6 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code Block 1"
      ],
      "metadata": {
        "id": "fv7XCV6DRmRR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import openai\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "QlkldrBdzaMC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCoxlgoCSD2P",
        "outputId": "7fa15c71-e4b1-4414-9e97-3ea3a1adca76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API key loaded from Colab userdata.\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    # Try to get API key from Google Colab's userdata\n",
        "\n",
        "    # Make sure to replace 'OPENAI_API_KEY1' with the actual name you gave your secret key in Google Colab secrets.\n",
        "    api_key = userdata.get('AITutorCourseKey')\n",
        "    if api_key:\n",
        "        print(\"API key loaded from Colab userdata.\")\n",
        "\n",
        "    if not api_key:\n",
        "        print(\"OpenAI API key not found in Colab secrets.\")\n",
        "        api_key = input(\"Please enter your OpenAI API key manually: \")\n",
        "\n",
        "except ImportError:\n",
        "    print(\"Not running in Colab environment.\")\n",
        "    api_key = input(\"Please enter your OpenAI API key manually: \")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Final API_KEY validation\n",
        "if not api_key:\n",
        "    raise ValueError(\"API Key not provided. Please ensure it's set.\")\n",
        "else:\n",
        "    print(f\"API Key loaded successfully (starting with: {api_key[:4]}...).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hd80wdPFzErB",
        "outputId": "85e18e2f-8ef6-4c17-e6ba-a7423cdce1e2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API Key loaded successfully (starting with: sk-p...).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# All subsequent API calls will be made through this 'client' object.\n",
        "try:\n",
        "    client = openai.OpenAI(api_key=api_key)\n",
        "    print(\"OpenAI client initialized successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error initializing OpenAI client: {e}\")\n",
        "    # You might want to exit or raise the error here depending on desired behavior\n",
        "    raise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCTwrKEOTEc1",
        "outputId": "d1aea5fa-5d3b-439f-93d8-bcf6b94189d1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI client initialized successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code Block 2"
      ],
      "metadata": {
        "id": "AEJwU3sUUTMk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Block 2: First Turn - Asking the Initial Question ---\n",
        "print(\"\\n--- Starting Conversation: Turn 1 ---\")\n",
        "\n",
        "# Define the system message (persona) for the AI Tutor\n",
        "system_message = {\"role\": \"system\", \"content\": \"You are a helpful AI Tutor explaining Large Language Model concepts simply.\"}\n",
        "\n",
        "# Define the user's first question\n",
        "user_message_1 = {\"role\": \"user\", \"content\": \"Can you explain what 'tokens' are in the context of LLMs, like I'm new to this?\"}\n",
        "\n",
        "# Create the messages list for the *first* API call\n",
        "messages_history = [\n",
        "    system_message,\n",
        "    user_message_1\n",
        "]\n",
        "\n",
        "print(f\"Sending messages: {messages_history}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQwTVEM3TFeL",
        "outputId": "c0dfe155-eb1c-404b-b220-c89e6c149a2a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting Conversation: Turn 1 ---\n",
            "Sending messages: [{'role': 'system', 'content': 'You are a helpful AI Tutor explaining Large Language Model concepts simply.'}, {'role': 'user', 'content': \"Can you explain what 'tokens' are in the context of LLMs, like I'm new to this?\"}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define parameters for this call\n",
        "MODEL = \"gpt-4o-mini\"\n",
        "TEMPERATURE = 1.0\n",
        "MAX_TOKENS = 150\n",
        "SEED = 123"
      ],
      "metadata": {
        "id": "Tjoa5o1nzsH7"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    print(f\"\\nMaking API call to {MODEL}...\")\n",
        "    # Use the client object's method to create a chat completion\n",
        "    completion_1 = client.chat.completions.create(\n",
        "        model=MODEL,\n",
        "        messages=messages_history,\n",
        "        temperature=TEMPERATURE,\n",
        "        max_tokens=MAX_TOKENS,\n",
        "        seed=SEED\n",
        "    )\n",
        "    print(\"API call successful.\")\n",
        "\n",
        "    # --- Process the response from the first turn ---\n",
        "    # Extract the assistant's reply content\n",
        "    assistant_response_1 = completion_1.choices[0].message.content\n",
        "    # Extract the full message object to add to history later\n",
        "    assistant_message_1 = completion_1.choices[0].message\n",
        "\n",
        "    print(\"\\nAI Tutor (Turn 1):\")\n",
        "    print(assistant_response_1)\n",
        "\n",
        "    # Print token usage for this call\n",
        "    usage_1 = completion_1.usage\n",
        "    print(f\"\\nToken Usage (Turn 1): Prompt={usage_1.prompt_tokens}, Completion={usage_1.completion_tokens}, Total={usage_1.total_tokens}\")\n",
        "    finish_reason_1 = completion_1.choices[0].finish_reason\n",
        "    print(f\"Finish Reason: {finish_reason_1}\")\n",
        "\n",
        "except openai.APIError as e:\n",
        "    # Handle API errors (e.g., server issues, rate limits)\n",
        "    print(f\"OpenAI API returned an API Error: {e}\")\n",
        "except openai.AuthenticationError as e:\n",
        "    # Handle Authentication errors (e.g., invalid API key)\n",
        "    print(f\"OpenAI Authentication Error: {e}\")\n",
        "except Exception as e:\n",
        "    # Handle other potential errors\n",
        "    print(f\"An unexpected error occurred: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rCEBIxfWHrX",
        "outputId": "f3488e08-daf9-44f6-a0c0-2986976b3aec"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Making API call to gpt-4o-mini...\n",
            "API call successful.\n",
            "\n",
            "AI Tutor (Turn 1):\n",
            "Yes, that's correct! Common words, often called stop words, like \"the,\" \"is,\" \"and,\" and \"but,\" are typically treated as single tokens. Most language models recognize them as whole words since they appear frequently in text.\n",
            "\n",
            "However, tokens can vary depending on the specific tokenization method used by the model. For example, some models might break longer or less common words into smaller parts, while they keep common words as single tokens. But generally, short and frequently used words like \"the\" or \"is\" will be one token each. \n",
            "\n",
            "So, to sum it up: yes, common words are usually single tokens, but the way text is tokenized can differ among different models!\n",
            "\n",
            "Token Usage (Turn 1): Prompt=228, Completion=143, Total=371\n",
            "Finish Reason: stop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code Block 3"
      ],
      "metadata": {
        "id": "H8ILfDt3m-Ok"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Block 3: Second Turn - Asking a Follow-up Question ---\n",
        "print(\"\\n--- Continuing Conversation: Turn 2 ---\")\n",
        "\n",
        "# Assume the first turn was successful and we have 'assistant_message_1'\n",
        "# Define the user's second question, referencing the previous explanation\n",
        "user_message_2 = {\"role\": \"user\", \"content\": \"Thanks! So, based on your explanation, are common words like 'the' or 'is' usually single tokens?\"}\n",
        "\n",
        "# --- CRITICAL STEP: Update the message history ---\n",
        "# Append the assistant's *previous* response to the history\n",
        "messages_history.append(assistant_message_1)\n",
        "# Append the user's *new* question to the history\n",
        "messages_history.append(user_message_2)\n",
        "\n",
        "print(f\"\\nSending updated messages: {messages_history}\") # Notice how the list has grown"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCQN9unP2an6",
        "outputId": "d2944099-1ef7-4a1a-81ad-f2127fbb3c16"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Continuing Conversation: Turn 2 ---\n",
            "\n",
            "Sending updated messages: [{'role': 'system', 'content': 'You are a helpful AI Tutor explaining Large Language Model concepts simply.'}, {'role': 'user', 'content': \"Can you explain what 'tokens' are in the context of LLMs, like I'm new to this?\"}, ChatCompletionMessage(content='Sure! In the context of Large Language Models (LLMs), a \"token\" is a basic unit of text that the model processes. Tokens can be words, parts of words, or even punctuation marks. \\n\\nThink of it this way: when you write a sentence, it’s made up of words. However, for an LLM, it breaks down the text into smaller pieces called tokens. For example, the sentence “I love cats!” might be broken down into the tokens: “I”, “love”, “cats”, and “!”.\\n\\nHere are a few key points to help you understand tokens better:\\n\\n1. **Tokenization**: This is the process of converting text into tokens. Different models might use different methods to decide how', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None), {'role': 'user', 'content': \"Thanks! So, based on your explanation, are common words like 'the' or 'is' usually single tokens?\"}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters for the second call (could be the same or different)\n",
        "# Let's make it slightly more deterministic for a factual answer\n",
        "TEMPERATURE_2 = 0.2\n",
        "MAX_TOKENS_2 = 100\n",
        "# Using the same seed ensures the *entire conversation flow* is reproducible if inputs are identical\n",
        "SEED_2 = 123"
      ],
      "metadata": {
        "id": "Dqc6-OrN2kzZ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    print(f\"\\nMaking API call to {MODEL} (Turn 2)...\")\n",
        "    completion_2 = client.chat.completions.create(\n",
        "        model=MODEL,\n",
        "        messages=messages_history, # Send the *full* history\n",
        "        temperature=TEMPERATURE_2,\n",
        "        max_tokens=MAX_TOKENS_2,\n",
        "        seed=SEED_2\n",
        "    )\n",
        "    print(\"API call successful.\")\n",
        "\n",
        "    # --- Process the response from the second turn ---\n",
        "    assistant_response_2 = completion_2.choices[0].message.content\n",
        "    # We don't strictly need to save assistant_message_2 unless continuing the conversation\n",
        "\n",
        "    print(\"\\nAI Tutor (Turn 2):\")\n",
        "    print(assistant_response_2)\n",
        "\n",
        "    # Print token usage for this call\n",
        "    usage_2 = completion_2.usage\n",
        "    print(f\"\\nToken Usage (Turn 2): Prompt={usage_2.prompt_tokens}, Completion={usage_2.completion_tokens}, Total={usage_2.total_tokens}\")\n",
        "    # Note: prompt_tokens for turn 2 will be larger as it includes the history from turn 1.\n",
        "    finish_reason_2 = completion_2.choices[0].finish_reason\n",
        "    print(f\"Finish Reason: {finish_reason_2}\")\n",
        "\n",
        "except openai.APIError as e:\n",
        "    print(f\"OpenAI API returned an API Error: {e}\")\n",
        "except openai.AuthenticationError as e:\n",
        "    print(f\"OpenAI Authentication Error: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkzqFe8BWJHc",
        "outputId": "f1e0140a-7868-4610-c8a7-7dcbf65130f3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Making API call to gpt-4o-mini (Turn 2)...\n",
            "API call successful.\n",
            "\n",
            "AI Tutor (Turn 2):\n",
            "Yes, that's correct! Common words like \"the\" or \"is\" are typically treated as single tokens in most tokenization systems. Since these words are frequent and straightforward, they are usually kept intact.\n",
            "\n",
            "However, it's important to note that the way text is tokenized can vary depending on the specific model and its tokenization method. Some models might break down less common words or longer words into smaller parts, especially if they are not frequently used. But for common words, they usually remain as single\n",
            "\n",
            "Token Usage (Turn 2): Prompt=228, Completion=100, Total=328\n",
            "Finish Reason: length\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Example usage object from completion_1 or completion_2:\n",
        "print(usage_1.prompt_tokens)  # -> number of input tokens\n",
        "print(usage_1.completion_tokens) # -> number of output tokens\n",
        "print(usage_1.total_tokens) # -> sum of both"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFh2GfScWT-N",
        "outputId": "b898e803-4192-44e4-dec8-0d272b009669"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "46\n",
            "150\n",
            "196\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code Block 4"
      ],
      "metadata": {
        "id": "N0RI6UGhoDtC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Block 4: Cost Calculation Function & Example ---\n",
        "\n",
        "def calculate_cost(usage, input_price_per_mil, output_price_per_mil):\n",
        "    \"\"\"Calculates the cost of an API call based on token usage and prices.\n",
        "\n",
        "    Args:\n",
        "        usage: The usage object from the OpenAI completion response\n",
        "               (e.g., completion.usage). It should have attributes\n",
        "               'prompt_tokens' and 'completion_tokens'.\n",
        "        input_price_per_mil: Cost in USD per 1 million input tokens.\n",
        "        output_price_per_mil: Cost in USD per 1 million output tokens.\n",
        "\n",
        "    Returns:\n",
        "        The total cost in USD for the API call, or None if usage is invalid.\n",
        "    \"\"\"\n",
        "    if not usage or not hasattr(usage, 'prompt_tokens') or not hasattr(usage, 'completion_tokens'):\n",
        "        print(\"Warning: Invalid usage object provided for cost calculation.\")\n",
        "        return None\n",
        "\n",
        "    input_cost = (usage.prompt_tokens / 1_000_000) * input_price_per_mil\n",
        "    output_cost = (usage.completion_tokens / 1_000_000) * output_price_per_mil\n",
        "    total_cost = input_cost + output_cost\n",
        "    return total_cost"
      ],
      "metadata": {
        "id": "EJa3-qoE2trJ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Current Prices (April 2025) for GPT-4o-mini ---\n",
        "# IMPORTANT: Always verify at https://openai.com/pricing\n",
        "PRICE_INPUT_PER_MIL = 0.60\n",
        "PRICE_OUTPUT_PER_MIL = 2.40\n",
        "\n",
        "print(f\"\\n--- Cost Calculations (GPT-4o-mini, April 2025 Rates) ---\")\n",
        "print(f\"Prices: Input=${PRICE_INPUT_PER_MIL:.2f}/1M, Output=${PRICE_OUTPUT_PER_MIL:.2f}/1M\")"
      ],
      "metadata": {
        "id": "U1xFpuBa2yl8",
        "outputId": "6011c6ba-850d-4ccd-b401-44004c32977f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Cost Calculations (GPT-4o-mini, April 2025 Rates) ---\n",
            "Prices: Input=$0.60/1M, Output=$2.40/1M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate cost for Turn 1 (assuming completion_1 and usage_1 exist from Block 2)\n",
        "try:\n",
        "    if 'usage_1' in locals(): # Check if usage_1 variable exists\n",
        "         cost_1 = calculate_cost(usage_1, PRICE_INPUT_PER_MIL, PRICE_OUTPUT_PER_MIL)\n",
        "         if cost_1 is not None:\n",
        "              print(f\"\\nCost for Turn 1:\")\n",
        "              print(f\"  Prompt Tokens: {usage_1.prompt_tokens}, Completion Tokens: {usage_1.completion_tokens}\")\n",
        "              print(f\"  Total Cost: ${cost_1:.8f}\")\n",
        "    else:\n",
        "         print(\"\\nSkipping Turn 1 cost calculation (usage_1 not found).\")\n",
        "\n",
        "    # Calculate cost for Turn 2 (assuming completion_2 and usage_2 exist from Block 3)\n",
        "    if 'usage_2' in locals(): # Check if usage_2 variable exists\n",
        "        cost_2 = calculate_cost(usage_2, PRICE_INPUT_PER_MIL, PRICE_OUTPUT_PER_MIL)\n",
        "        if cost_2 is not None:\n",
        "            print(f\"\\nCost for Turn 2:\")\n",
        "            print(f\"  Prompt Tokens: {usage_2.prompt_tokens}, Completion Tokens: {usage_2.completion_tokens}\")\n",
        "            print(f\"  Total Cost: ${cost_2:.8f}\")\n",
        "    else:\n",
        "         print(\"\\nSkipping Turn 2 cost calculation (usage_2 not found).\")\n",
        "\n",
        "    # Calculate total conversation cost\n",
        "    if 'cost_1' in locals() and 'cost_2' in locals() and cost_1 is not None and cost_2 is not None:\n",
        "        total_conversation_cost = cost_1 + cost_2\n",
        "        print(f\"\\nTotal Conversation Cost (Turn 1 + Turn 2): ${total_conversation_cost:.8f}\")\n",
        "\n",
        "except NameError as e:\n",
        "    print(f\"\\nCould not calculate costs, a required variable is missing: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during cost calculation: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "217IT8sKWoAK",
        "outputId": "ab4a6e65-8682-44c7-c8a6-241b055aead4"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Cost for Turn 1:\n",
            "  Prompt Tokens: 228, Completion Tokens: 143\n",
            "  Total Cost: $0.00048000\n",
            "\n",
            "Cost for Turn 2:\n",
            "  Prompt Tokens: 228, Completion Tokens: 100\n",
            "  Total Cost: $0.00037680\n",
            "\n",
            "Total Conversation Cost (Turn 1 + Turn 2): $0.00085680\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vzk2VbYfW8sx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}